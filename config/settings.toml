# CAM Configuration
# Edit values here. Reload from the dashboard without restarting.
# Environment variables override: CAM_<SECTION>_<KEY> (e.g. CAM_DASHBOARD_PORT=9090)

[dashboard]
host = "0.0.0.0"
port = 8080
log_level = "info"
heartbeat_check_interval = 10      # seconds between heartbeat sweeps
heartbeat_timeout = 120             # fallback offline threshold (seconds)
agent_dispatch_timeout = 120        # max wait for agent task response (seconds)
default_event_count = 200           # events sent to dashboard on connect

[orchestrator]
poll_interval = 1.0                 # seconds between queue checks when idle

[health]
heartbeat_interval = 30.0           # expected seconds between agent heartbeats
missed_heartbeat_threshold = 3      # consecutive misses before marking offline
success_rate_threshold = 75.0       # below this % -> yellow health indicator

[events]
max_events = 1000                   # in-memory event log capacity

[analytics]
db_path = "data/analytics.db"

[models]
ollama_url = "http://localhost:11434"

[models.routing]
simple = "glm-4.7-flash"
routine = "gpt-oss:20b"
agentic = "kimi-k2.5"
complex = "claude"
nuanced = "claude"
# Tier-based routing (auto-classifier → task_classifier.py)
# Tier 1 = small/fast, Tier 2 = medium, Tier 3 = large/reasoning
tier1 = "phi4-mini:3.8b"          # small/fast local model for simple tasks
tier2 = "gpt-oss:20b"
tier3 = "glm-4.7-flash"

[models.costs.claude]
input = 3.00                        # USD per million input tokens
output = 15.00                      # USD per million output tokens

[models.costs."kimi-k2.5"]
input = 0.50
output = 1.50

[commands]
status_report = 30
system_info = 30
restart_service = 60
run_diagnostic = 90
capture_sensor_data = 45

[connector]
heartbeat_interval = 30
reconnect_delay = 3
dashboard_host = "192.168.88.4"
dashboard_port = 8080
systemctl_timeout = 30

[notifications]
enabled = true                      # master toggle for the notification system
max_history = 200                   # max notifications kept in memory
agent_disconnect = true             # alert when an agent goes offline
task_failure = true                 # alert when a task fails
kill_switch = true                  # alert when the kill switch is activated
high_error_rate = true              # alert when task error rate exceeds threshold
error_rate_threshold = 50.0         # error rate % that triggers an alert
error_rate_window = 20              # number of recent tasks to evaluate
cost_threshold = true               # alert when cumulative model cost exceeds limit
cost_threshold_usd = 1.00           # USD threshold for cost alert

[file_transfer]
chunk_size = 65536                  # bytes per chunk before base64 encoding
max_file_size = 52428800            # 50 MB max file size
receive_dir = "data/transfers"      # server-side directory for received files
agent_receive_dir = "~/receive"     # default agent-side receive directory
max_active_transfers = 5            # max concurrent transfers
history_size = 100                  # completed transfer history ring buffer

[scheduler]
check_interval = 30                 # seconds between schedule checks
persist_file = "data/schedules.json"  # schedule persistence file

[memory]
short_term_max_messages = 200       # max messages before summarization kicks in
short_term_summary_ratio = 0.5      # keep newest 50% of messages when summarizing
working_memory_path = "data/tasks/working_memory.json"  # persistent task state file
long_term_persist_dir = "data/memory/chromadb"           # ChromaDB vector store location
long_term_collection = "cam_long_term"                   # ChromaDB collection name
long_term_seed_file = "CAM_BRAIN.md"                     # seed knowledge file for first startup
episodic_db_path = "data/memory/episodic.db"              # SQLite database for conversation history
episodic_retention_days = 365                              # how long to keep episodic records

[api]
api_key = ""                        # Set a key to require X-API-Key header. Empty = open access.

[telegram]
bot_token = ""                      # From @BotFather. Empty = bot disabled.
allowed_chat_ids = []               # George's chat_id(s). Integer list.
poll_interval = 0.5                 # seconds between task result checks
task_timeout = 120.0                # seconds to wait for task result

[tts]
audio_dir = "data/audio"                    # where synthesized WAV files are stored
voices_dir = "data/audio/voices"            # where Piper .onnx voice models live
default_voice = "en_US-lessac-medium"       # Piper default voice model
sample_rate = 22050                         # audio sample rate in Hz

[content_calendar]
db_path = "data/content_calendar.db"  # SQLite database for content pipeline

[research]
db_path = "data/research.db"           # SQLite database for research results
max_pages_per_query = 5                 # max pages to fetch per research task
request_timeout = 10                    # HTTP request timeout in seconds
rate_limit_delay = 0.5                  # seconds between web requests

[scout]
db_path = "data/scout.db"
scan_interval = 3600                    # seconds between automatic scans
score_threshold = 8                     # minimum score for hot deal notification
makes = ["honda", "yamaha", "suzuki", "kawasaki", "harley-davidson", "ducati"]
models = []
year_min = 0
year_max = 0
price_min = 0
price_max = 5000
location = "Portland OR"
radius_miles = 50
keywords = []
exclude_keywords = ["parts only"]

[business]
db_path = "data/business.db"
seed_sample_data = true
default_labor_rate = 75.0
invoice_prefix = "DC"

[auth]
username = "george"
password_hash = ""                         # empty = auth disabled (open access). Run: python -m security.set_password
session_timeout = 3600                     # 1 hour inactivity timeout
max_login_attempts = 5
lockout_duration = 300                     # 5 min lockout after max failures

[security]
audit_db_path = "data/security_audit.db"  # SQLite database for security audit trail
approval_timeout = 30                      # seconds before auto-reject (silence ≠ consent)
log_tier1_actions = true                   # log autonomous actions to audit trail

[backup]
backup_dir = "data/backups"               # where backup archives are stored
max_backups = 10                           # number of backups to keep (oldest rotated out)
daily_backup_time = "03:00"               # time for scheduled daily backup (HH:MM, UTC)

[message_bus]
max_messages = 500                         # per-channel ring buffer capacity

[context]
rotation_threshold = 0.9                   # rotate at 90% of context limit
ltm_top_k = 3                             # max long-term memory results per query
ltm_min_score = 0.3                       # minimum relevance score for LTM results
episodic_recent_count = 5                 # recent episodes to include in context
max_working_memory_tasks = 10             # max active tasks shown in context
token_estimate_divisor = 4                # chars-per-token heuristic (len // this)

[context.limits]
"glm-4.7-flash" = 128000
"gpt-oss:20b" = 32000
"phi4-mini:3.8b" = 8000
"claude" = 200000
"kimi-k2.5" = 128000

[webhooks]
enabled = true
db_path = "data/webhooks.db"
max_retries = 5
retry_base_seconds = 10
retry_max_seconds = 3600
inbound_secret = ""

[reports]
db_path = "data/reports.db"
pdf_dir = "data/reports"
daily_time = "06:00"
weekly_time = "07:00"
monthly_time = "08:00"

[email]
# Future: SMTP email delivery of reports
enabled = false
smtp_host = ""
smtp_port = 587
smtp_user = ""
smtp_password = ""
from_address = "cam@dopplercycles.com"
to_address = ""
use_tls = true

[appointments]
db_path = "data/appointments.db"
home_lat = 45.4976
home_lon = -122.4302
avg_speed_mph = 30.0
road_factor = 1.4
reminder_check_interval = 300
default_duration = 60

[knowledge]
db_path = "data/knowledge_ingest.db"
inbox_dir = "data/knowledge/inbox"
processed_dir = "data/knowledge/processed"
scan_interval = 30
max_file_size = 10485760
chunk_target_size = 1000
chunk_overlap = 100
